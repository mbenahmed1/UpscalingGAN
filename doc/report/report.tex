\documentclass[11pt,twocolumn,twoside,paper=a4]{IEEEtran}
\usepackage{amsmath}
% Swap the comments on the two below lines to toggle the geometry view of the margins, etc...
%\usepackage[margin=0.75in,headheight=0.45in,showframe]{geometry}
\usepackage[margin=0.75in,headheight=0.45in]{geometry}
\usepackage[pdftex]{epsfig}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage[utf8]{inputenc}
\include{graphicsx}
\usepackage{graphicx}
\usepackage{subcaption}

\pagestyle{fancy}
%\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\rhead{\includegraphics[height=0.4in]{../images/uos.pdf}}
\fancyhead[LO]{\sc Upscaling GAN} % shorter form of title to fit in space
\fancyhead[LE]{\sc ben Ahmed, Hoffmann, Zielinski} % author list or et al., to fit in space
\chead{}
\cfoot{}

\begin{document}
\title{\vspace{0.2in}\sc Upscaling images using a Generative Adversarial Network}
\author{Martin ben Ahmed$^{1,2}$\thanks{$^1$Institute of Computer Science, University of Osnabr√ºck $^2$mbenahmed@uos.de $^3$phoffmann@uos.de $^4$szielinski@uos.de}, Patrick Hoffmann$^{1,3}$, Sebastian Zielinski$^{1,4}$}

\maketitle
\thispagestyle{fancy}
\begin{abstract}
New state of the art neural network architectures and ever increasing computational resources
offer a large potential for new applications of deep learning in real life problems. The omnipresent
availability of historical image material and the use of advanced deep learning techniques created a desire
to use written of image data in a new way. Since resolution in historical image data is often very limited
researchers have developed Generative Adversarial Network (GAN) models to increase resolution in order to make old
data usable. In this report we describe our approach on recreating a GAN for image super resolution, heavily
inspired by Ledig et al.~\cite{DBLP:journals/corr/LedigTHCATTWS16}.
\end{abstract}

\section{Motivation}
TODO

\section{Method}
\subsection{Network architecture}
The network architecture shown in figure~\ref{fig:network_architecture} is also heavily inspired by Ledit et al.~\cite{DBLP:journals/corr/LedigTHCATTWS16}.\\
The generator starts by utilizing a convolutional layer activated by PReLU. 
After that, it is followed by a series of residual blocks, 
which aim to remember the input of the network. 
After another convolutional layer the input will be added to the current state by a skip connection spanning over all residual blocks. 
Finally, the network uses a series of upscaling blocks, consisting of convolutional and conv. transpose layers followed by PReLU activation to perform the actuall upscaling process. 
Afterwards, another convolutional layer will be applied and generate the output image. The generator tries to recreate the full resolution image as exactly as possible from the low resolution image it gets as an input.

The discriminator starts with the same convolutional layers as the generator, activated by Leaky ReLU instead of PReLU. 
Afterwards it is followed by a series of convolution, batch normalization and Leaky ReLU activation blocks, varying in number of channels and stride size. 
Finally the images will run through a dense layer with the size of 1024, activated by Leaky ReLU and a dense layer of size 1, activated by Sigmoid. 
The discriminator then decides whether the input image was upscaled by the generator or is an original full resolution image.

All convolutional layers are described by a code consisting of k (kernel size), n (number of blocks) and s (stride size). Example: k9-n64-s1 describes a layer with a kernel size of 9x9, 64 channels and a stride of 1.

\begin{center}
    \begin{figure}[h] 
        \includegraphics[scale=0.7]{../images/gen_and_dis.pdf}  
        \caption{Generator and Discriminator Network similar to Ledig et al.~\cite{DBLP:journals/corr/LedigTHCATTWS16} }   
        \label{fig:network_architecture}
    \end{figure} 
\end{center}


\subsection{Dataset}
We use the Common Objects in Context Dataset (COCO) by Microsoft~\cite{lin2015microsoft}.
\section{Experiments}
TODO

\section*{Discussion}
TODO

\section*{Conclusion}
TODO


\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}
