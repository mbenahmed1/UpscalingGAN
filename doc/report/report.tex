\documentclass[11pt,twocolumn,twoside,paper=a4]{IEEEtran}
\usepackage{amsmath}
% Swap the comments on the two below lines to toggle the geometry view of the margins, etc...
%\usepackage[margin=0.75in,headheight=0.45in,showframe]{geometry}
\usepackage[margin=0.75in,headheight=0.45in]{geometry}
\usepackage[pdftex]{epsfig}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage[utf8]{inputenc}
\include{graphicsx}
\usepackage{graphicx}
\usepackage{subcaption}

\pagestyle{fancy}
%\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\rhead{\includegraphics[height=0.4in]{../images/uos.pdf}}
\fancyhead[LO]{\sc Upscaling GAN} % shorter form of title to fit in space
\fancyhead[LE]{\sc ben Ahmed, Hoffmann, Zielinski} % author list or et al., to fit in space
\chead{}
\cfoot{}

\begin{document}
\title{\vspace{0.2in}\sc Upscaling images using a Generative Adversarial Network}
\author{Martin ben Ahmed$^{1,2}$\thanks{$^1$Institute of Computer Science, University of Osnabr√ºck $^2$mbenahmed@uos.de $^3$pahoffmann@uos.de $^4$szielinski@uos.de}, Patrick Hoffmann$^{1,3}$, Sebastian Zielinski$^{1,4}$}

\maketitle
\thispagestyle{fancy}
\begin{abstract}
New state of the art neural network architectures and ever increasing computational resources
offer a large potential for new applications of deep learning in real life problems.
In the context of images, GANs play a big role in creating new plausible sample data from a specified input.
Amongst others, GANs can solve the problem of upscaling an image while increasing its level of detail, making the image look better, when enlarging it. Upscaling is especially useful in context of video games, surveillance cameras or old data with a very low resolution.
In this report we describe our approach on recreating a GAN for image super resolution, heavily
inspired by Ledig et al.~\cite{DBLP:journals/corr/LedigTHCATTWS16}.
\end{abstract}

\section{Motivation}

With TV's and screens getting bigger and an ever increasing resolution the computitional effort to render images for these resolutions rises. Especially in the context of video games, powerful GPU's are needed to render images in high resolutions fast enough. While today most of these graphics cards are really expensive, most of the time they are sold out and it is hard to get a hand on one of them.

A solution to this problem is to render frames in a lower quality and upscale them to a bigger resolution in constant time using a neural network, typically utilizing a GAN architecture \cite{watson2020deep}.
This upscaling is also reffered to as Super-Resolution.
As a general upscaling would keep the level of detail the same and therefore visibly reduce the quality of the image when viewed on a larger screen with higher resolution, the GAN is supposed to increase the resolution while also increasing the level of detail, which is not possible with computer vision algorithms.
The idea is, that the GAN creates an intuition on how to add detail to scenes during training.

This reduces the time to render frames drastically, which makes it possible to play games on high resolution without the need to buy the most powerful graphics card on the market. 
This technique also proves useful in many other szenarios.
Using upscaling, images with low resolution (e.g. historical data like old fotos or films) can be reprocessed, increasing its usability and esthetic.
Further, data from surveillance cameras, which typically come at a low resolution, especially when they are older, can be made more useful, as details are increased. Therefore it can be also used to help in crime investigation, especially with further postprocessing steps, like face or object recognition.

Though there are many different approach on Super-Resolution like using a ResNet architecture \cite{lim2017enhanced}, a Depp Laplacian Pyramid Network \cite{lai2018fast} or a Sub-Pixel Convolutional Neural Network \cite{shi2016real}, we choose to use the GAN architecture proposed in \cite{DBLP:journals/corr/LedigTHCATTWS16}, as it states to deliver extraordinary results.
In the following, we describe our recreation of this approach.

\section{Method}
\subsection{Network architecture}
The network architecture shown in figure~\ref{fig:network_architecture} is also heavily inspired by Ledit et al.~\cite{DBLP:journals/corr/LedigTHCATTWS16}.\\
The generator starts by utilizing a convolutional layer activated by PReLU. 
After that, it is followed by a series of residual blocks, 
which aim to remember the input of the network. 
After another convolutional layer the input will be added to the current state by a skip connection spanning over all residual blocks. 
Finally, the network uses a series of upscaling blocks, consisting of convolutional and conv. transpose layers followed by PReLU activation to perform the actuall upscaling process. 
Afterwards, another convolutional layer will be applied and generate the output image. The generator tries to recreate the full resolution image as exactly as possible from the low resolution image it gets as an input.

The discriminator starts with the same convolutional layers as the generator, activated by Leaky ReLU instead of PReLU. 
Afterwards it is followed by a series of convolution, batch normalization and Leaky ReLU activation blocks, varying in number of channels and stride size. 
Finally the images will run through a dense layer with the size of 1024, activated by Leaky ReLU and a dense layer of size 1, activated by Sigmoid. 
The discriminator then decides whether the input image was upscaled by the generator or is an original full resolution image.

All convolutional layers are described by a code consisting of k (kernel size), n (number of blocks) and s (stride size). Example: k9-n64-s1 describes a layer with a kernel size of 9x9, 64 channels and a stride of 1.

\begin{center}
    \begin{figure}[h] 
        \includegraphics[scale=0.7]{../images/gen_and_dis.pdf}  
        \caption{Generator and Discriminator Network similar to Ledig et al.~\cite{DBLP:journals/corr/LedigTHCATTWS16} }   
        \label{fig:network_architecture}
    \end{figure} 
\end{center}


\subsection{Dataset}
We use the Common Objects in Context Dataset (COCO) by Microsoft~\cite{lin2015microsoft}.
\section{Experiments}
TODO

\section*{Discussion}
TODO

\section*{Conclusion}
TODO


\bibliographystyle{ieeetr}
\bibliography{references}

\end{document}
