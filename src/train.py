"""Contains the training loop and preparation routines.

This file contains the main training loop and all other routines for loading
and preparing train data.
"""

# imports
import tensorflow as tf
import utils
import model
import config
import os
import sys
import matplotlib.pyplot as plt
import time
from datetime import datetime
from pathlib import Path


# disabling gpu if needed
if not config.USEGPU:
    os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

# memory growth
devices = tf.config.list_physical_devices('GPU')
try:
    tf.config.experimental.set_memory_growth(devices[0], True)
except:
    print('Could not enable memory growth.')


bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)


def generator_loss(d_true, d_pred):
    """
    d_pred: discriminator's prediction  of the image generated by the generator  
    take cross entropy between predicted label and all labels as 1s
    because we want to minimize the difference between them
    --> the more the discriminator thinks the images are real, the better our generator
    """
    return tf.keras.losses.MSE(d_true, d_pred)


def discriminator_loss(real_img_lbl, fake_img_lbl):
    """
    real_img_lbl: labels that discriminator predicted when seeing real images (should ideally be all 1s)
    fake_img_lbl: labels that discriminator predicted when seeing fake images (should ideally be all 0s)
    """
    real_loss = bce(tf.ones(real_img_lbl.shape), real_img_lbl)
    fake_loss = bce(tf.zeros(fake_img_lbl.shape), fake_img_lbl)

    return real_loss + fake_loss


def train(dataset, low_res_image, full_res_image, epochs):

    # saving low res sample image to weights folder
    plt.imshow(low_res_image[0, :, :, :])
    plt.savefig("../seq/low_res_image.pdf")
    
    # saving full res sample image to weights folder
    plt.imshow(full_res_image[0, :, :, :])
    plt.savefig("../seq/full_res_image.pdf")
    
    # run training steps for number of epochs
    for epoch in range(epochs):
        start = time.time()
        
        for data in dataset:
            (loss1, loss2) = train_step(data)
            loss_gen.append(loss1)
            loss_desc.append(loss2)

        # save weights every CHECKPOINTINTERVAL time
        if epoch % config.CHECKPOINTINTERVAL == 0:
            generator.save("../seq/gen")
            discriminator.save("../seq/dis")

        # print time elapsed for this particular epoch
        time_per_epoch = int(time.time() - start)
        epoch_time_string = (f'Time for epoch {epoch} is {time_per_epoch} s')
        print(epoch_time_string)

        # write time elapsed for this epoch to file
        text_file = open("../seq/log.txt", "a")
        text_file.write(f'Time e_{epoch}:       {time_per_epoch} s \n')
        text_file.close()

        # generate one upscaled image from low res sample and write to folder
        upscaled_image = generator(low_res_image, training=False)
        plt.imshow(upscaled_image[0, :, :, :])
        plt.savefig("../seq/upscaled.pdf")

@tf.function
def train_step(images):
    image_lowRes = images[0]
    images_highRes = images[1]

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_image = generator(image_lowRes, training=True)

        real_output = discriminator(images_highRes, training=True)
        fake_output = discriminator(generated_image, training=True)

        gen_loss = generator_loss(images_highRes, generated_image)
        disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(
        gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(
        disc_loss, discriminator.trainable_variables)
    

    generator_optimizer.apply_gradients(
        zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(
        zip(gradients_of_discriminator, discriminator.trainable_variables))

    return (gen_loss, disc_loss)

with tf.device('/cpu:0'):

    # Create a generator
    rng = tf.random.Generator.from_seed(123, alg='philox')
    seed = rng.make_seeds(2)[1]

    # loading paths
    list_ds = tf.data.Dataset.list_files(config.DATAPATH)

    # loading and preparing images
    ds = list_ds.map(utils.prepare_images, config.NUMPARALLELCALLS)

    # size of the dataset
    size = len(list_ds)

    # TODO: make random from image to image
    # applying some augmentations for testing
    ds = ds.map(lambda x: utils.saturate(x, config.SATURATIONMIN,
                                        config.SATURATIONMAX, seed), config.NUMPARALLELCALLS)
    ds = ds.map(lambda x: utils.flip_left_right(x), config.NUMPARALLELCALLS)
    ds = ds.map(lambda x: utils.flip_up_down(x), config.NUMPARALLELCALLS)
    ds = ds.map(lambda x: utils.brighten(
        x, config.BRIGHTNESSMAXDETLA, seed), config.NUMPARALLELCALLS)
    ds = ds.map(lambda x: utils.contrast(x, config.CONTRASTMIN,
                                        config.CONTRASTMAX, seed), config.NUMPARALLELCALLS)

    # making pairs of the original and the scaled images
    ds = ds.map(utils.make_full_low_pairs, config.NUMPARALLELCALLS)
    ds = ds.take(int(size * config.DATASETSCALINGFACTOR))
    test_dataset = ds.take(int(size * config.TESTSPLITSIZE))
    train_dataset = ds.skip(int(size * config.TESTSPLITSIZE))

    # plot some samples

    if config.SHOWSAMPLES:
        for low_image, full_image in test_dataset.take(5):
            print(low_image.shape, full_image.shape)
            plt.imshow(low_image[0, :, :, :])
            plt.show()
            plt.imshow(full_image[0, :, :, :])
            plt.show()

    # batching
    test_dataset = test_dataset.batch(config.BATCHSIZE)
    train_dataset = train_dataset.batch(config.BATCHSIZE)

    # shuffling
    test_dataset = test_dataset.shuffle(buffer_size=config.BUFFERSIZE)
    train_dataset = train_dataset.shuffle(buffer_size=config.BUFFERSIZE)

    # prefetching
    train_dataset = train_dataset.prefetch(config.PREFETCHSIZE)
    test_dataset = test_dataset.prefetch(config.PREFETCHSIZE)


# *** TRAINING ***

# create optimizers
generator_optimizer = tf.keras.optimizers.Adam(config.GENLEARNINGRATE)
discriminator_optimizer = tf.keras.optimizers.Adam(config.DISLEARNINGRATE)

noise_dim = 100
num_examples_to_generate = 16

# create models
generator = model.Generator()
discriminator = model.Discriminator()
seed = tf.random.normal([num_examples_to_generate, noise_dim])

if len(sys.argv) > 1:
    generator = tf.keras.models.load_model(sys.argv[1])
    discriminator = tf.keras.models.load_model(sys.argv[2])
    #generator.load_weights(sys.argv[1])
    #discriminator.load_weights(sys.argv[2])

generator.compile(optimizer="adam", loss="mean_squared_error")
discriminator.compile(optimizer="adam", loss="binary_crossentropy")

loss_gen = []
loss_desc = []
print("")
print("")
print("Start training")

if config.SHOWSUMMARY:
    generator.build((config.BATCHSIZE, config.LOWIMAGESIZE, config.LOWIMAGESIZE, config.NUMCHANNELS))
    discriminator.build((config.BATCHSIZE, config.FULLIMAGESIZE, config.FULLIMAGESIZE, config.NUMCHANNELS))
    print("")
    print("")
    print(generator.summary())
    print(discriminator.summary())

full_res_image = []
low_res_image = []
for low, full in test_dataset.take(1):
    low_res_image = low
    full_res_image = full

train(test_dataset, low_res_image, full_res_image, config.EPOCHS)